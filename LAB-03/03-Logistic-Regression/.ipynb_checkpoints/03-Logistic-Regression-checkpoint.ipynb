{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Sunsun Kasajoo\"\n",
    "ID = \"st122283\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Logistic Regression\n",
    "\n",
    "Thus far, the problems we've encountered have been *regression* problems, in which the target $y \\in \\mathbb{R}$.\n",
    "\n",
    "Today we'll start experimenting with *classification* problems, beginning with *binary* classification problems, in which the target $y \\in \\{ 0, 1 \\}$.\n",
    "\n",
    "## Background\n",
    "\n",
    "The simplest approach to classification, applicable when the input feature vector $\\mathbf{x} \\in \\mathbb{R}^n$, is a simple generalization of what we\n",
    "do in linear regression. Recall that in linear regression, we assume that the target is drawn from a Gaussian distribution whose mean is a linear function\n",
    "of $\\mathbf{x}$:\n",
    "\n",
    "$$ y \\sim {\\cal N}(\\theta^\\top \\mathbf{x}, \\sigma^2) $$\n",
    "\n",
    "In logistic regression, similarly, we'll assume that the target is drawn from a Bernoulli distribution with parameter $p$ being the probability of\n",
    "class 1:\n",
    "\n",
    "$$ y \\sim \\text{Bernoulli}(p) $$\n",
    "\n",
    "That's fine, but how do we model the parameter $p$? How is it related to $\\mathbf{x}$?\n",
    "\n",
    "In linear regression, we assume that the mean of the Gaussian is $\\theta^\\top \\mathbf{x}$, i.e., a linear function of $\\mathbf{x}$.\n",
    "\n",
    "In logistic regression, we'll assume that $p$ is a \"squashed\" linear function of $\\mathbf{x}$, i.e.,\n",
    "\n",
    "$$ p = \\text{sigmoid}(\\theta^\\top \\mathbf{x}) = g(\\theta^\\top \\mathbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}. $$\n",
    "\n",
    "Later, when we introduce generalized linear models, we'll see why $p$ should take this form. For now, though, we can simply note that the selection makes\n",
    "sense. Since $p$ is a discrete probability, $p$ is bounded by $0 \\le p \\le 1$. The sigmoid function $g(\\cdot)$ conveniently obeys these bounds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp( -z ))\n",
    "\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "plt.plot(z, f(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "plt.title('Logistic sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sigmoid approaches 0 as its input approaches $-\\infty$ and approaches 1 as its input approaches $+\\infty$. If its input is 0, its value is 0.5.\n",
    "\n",
    "Again, this choice of function may seem strange at this point, but bear with it! We'll derive this function from a more general principle, the generalized\n",
    "linear model, later.\n",
    "\n",
    "OK then, we now understand that for logistic regression, the assumptions are:\n",
    "\n",
    "1. The *data* are pairs $(\\textbf{x}, y) \\in \\mathbb{R}^n \\times \\{ 0, 1 \\}$.\n",
    "1. The *hypothesis function* is $h_\\theta(\\textbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}$.\n",
    "\n",
    "What else do we need... ? A cost function and an algorithm for minimizing that cost function!\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "You can refer to the lecture notes to see the derivation, but for this lab, let's just skip to the chase.\n",
    "With the hypothesis $h_\\theta(\\mathbf{x})$ chosen as above, the log likelihood function $\\ell(\\theta)$ can be derived as\n",
    "$$ \\ell(\\theta) = \\log {\\cal L}(\\theta) =  \\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log(1 - (h_{\\theta}(\\mathbf{x}^{(i)})) .$$\n",
    "\n",
    "Negating the log likelihood function to obtain a loss function, we have\n",
    "\n",
    "$$ J(\\theta) = - \\sum_{i=1}^m y^{(i)}\\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(\\textbf{x}^{(i)})) .$$\n",
    "\n",
    "There is no closed-form solution to this problem like there is in linear regression, so we have to use gradient descent to find $\\theta$ minimizing $J(\\theta)$.\n",
    "Luckily, the function *is* convex in $\\theta$ so there is just a single global minimum, and gradient descent is guaranteed to get us there eventually if we take\n",
    "the right step size.\n",
    "\n",
    "The *stochastic* gradient of $J$, for a single observed pair $(\\mathbf{x}, y)$, turns out to be (see lecture notes)\n",
    "\n",
    "$$\\nabla_J(\\theta) = (h_\\theta(\\mathbf{x}) - y)\\mathbf{x} .$$\n",
    "\n",
    "Give some thought as to whether following this gradient to increase the loss $J$ would make a worse classifier, and vice versa!\n",
    "\n",
    "Finally, we obtain the update rule for the $j$th iteration selecting training pattern $i$:\n",
    "\n",
    "$$ \\theta^{(j+1)} \\leftarrow \\theta^{(j)} + \\alpha(y^{(i)} - h_\\theta(\\textbf{x}^{(i)}))\\textbf{x}^{(i)} .$$ \n",
    "\n",
    "Note that we can perform *batch gradient descent* simply by summing the single-pair gradient over the entire training set before taking a step,\n",
    "or *mini-batch gradient descent* by summing over a small subset of the data.\n",
    "\n",
    "## Example dataset 1: student admissions data\n",
    "\n",
    "This example is from Andrew Ng's machine learning course on Coursera.\n",
    "\n",
    "The data contain students' scores for two standardized tests and an admission decision (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam scores [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "-----------------------------\n",
      "Admission decision [0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load student admissions data. The data file does not contain headers,\n",
    "# so we use hard coded indices for exam 1, exam2, and the admission decision.\n",
    "\n",
    "data = np.loadtxt('ex2data1.txt',delimiter = ',')\n",
    "exam1_data = data[:,0]\n",
    "exam2_data = data[:,1]\n",
    "X = np.array([exam1_data, exam2_data]).T\n",
    "y = data[:,2]\n",
    "\n",
    "# Output some sample data\n",
    "\n",
    "print('Exam scores', X[0:5,:])\n",
    "print('-----------------------------')\n",
    "print('Admission decision', y[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "\n",
    "idx_0 = np.where(y == 0)\n",
    "idx_1 = np.where(y == 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(5, 5)) \n",
    "ax = plt.axes()\n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(exam1_data[idx_0], exam2_data[idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(exam1_data[idx_1], exam2_data[idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find good values for $\\theta$ without normalizing the data.\n",
    "We will definitely want to split the data into train and test, however..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# As usual, we fix the seed to eliminate random differences between different runs\n",
    "\n",
    "random.seed(12)\n",
    "\n",
    "# Partion data into training and test datasets\n",
    "\n",
    "m, n = X.shape\n",
    "XX = np.insert(X, 0, 1, axis=1)\n",
    "y = y.reshape(m, 1)\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important functions needed later\n",
    "\n",
    "Let's put all of our important functions here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):   \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def grad_j(X, y, y_pred):\n",
    "    return X.T @ (y - y_pred) / X.shape[0]\n",
    "    \n",
    "def j(theta, X, y):    \n",
    "    y_pred = h(X, theta)\n",
    "    error = (-y * np.log(y_pred)) - ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = sum(error) / X.shape[0]\n",
    "    grad = grad_j(X, y, y_pred)\n",
    "    return cost[0], grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta\n",
    "\n",
    "In any iterative algorithm, we need an initial guess. Here we'll just use zeros for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial theta: [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Initial predictions: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Targets: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize our parameters, and use them to make some predictions\n",
    "\n",
    "theta_initial = np.zeros((n+1, 1))\n",
    "\n",
    "print('Initial theta:', theta_initial)\n",
    "print('Initial predictions:', h(XX, theta_initial)[0:5,:])\n",
    "print('Targets:', y[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Here's a function to do batch training for `num_iters` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, theta_initial, alpha, num_iters):\n",
    "    theta = theta_initial\n",
    "    j_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost, grad = j(theta, X, y)\n",
    "        theta = theta + alpha * grad\n",
    "        j_history.append(cost)\n",
    "    return theta, j_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training\n",
    "\n",
    "Here we run the training function for a million batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1000000 iterations on full training set\n",
    "\n",
    "alpha = .0005\n",
    "num_iters = 1000000\n",
    "theta, j_history = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(\"Theta optimized:\", theta)\n",
    "print(\"Cost with optimized theta:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss curve\n",
    "\n",
    "Next let's plot the loss curve (loss as a function of iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-lab exercise from Example 1 (Total 35 points)\n",
    "\n",
    "That took a long time, right?\n",
    "\n",
    "We'll see if we can do better. We will try the following:\n",
    "\n",
    "1. Try increasing the learning rate $\\alpha$ and starting with a better initial $\\theta$. How much does it help?\n",
    "   - Try at least 2 learning rate $\\alpha$ with 2 difference $\\theta$ (4 experiments)\n",
    "   - Do not forget to plot the loss curve to compare your results\n",
    "\n",
    "2. Better yet, try *normalizing the data* and see if the training converges better. How did it go? \n",
    "   - Be sure to plot loss curves to compare the results with unnormalized and normalized data.\n",
    "\n",
    "3. Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report.\n",
    "\n",
    "Do this work in the following steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Fill in two different values for $\\alpha$ and $\\theta$.\n",
    "\n",
    "Use variable names `alpha1`, `alpha2`, `theta_initial1`, and `theta_initial2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "469661aa6773cd5305bcbca1054c82d7",
     "grade": false,
     "grade_id": "cell-98f6eb46f41c6686",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None' value to number(s) or function\n",
    "\n",
    "# declare your alphas\n",
    "alpha1 = 0.0008\n",
    "alpha2 = 0.001\n",
    "\n",
    "# initialize thetas as you want\n",
    "theta_initial1 = np.array([[16],[6],[9]])\n",
    "theta_initial2 = np.array([[15],[-5],[5]])\n",
    "\n",
    "# define your num iterations\n",
    "num_iters = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053e98ba0c27eb56eadb89857573b428",
     "grade": true,
     "grade_id": "cell-f9ecb4ab4402c8b4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_list = [alpha1, alpha2]\n",
    "print('alpha 1:', alpha1)\n",
    "print('alpha 2:', alpha2)\n",
    "\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "print('theta 1:', theta_initial_list[0])\n",
    "print('theta 2:', theta_initial_list[1])\n",
    "\n",
    "print('Use num iterations:', num_iters)\n",
    "\n",
    "# Test function: Do not remove\n",
    "assert alpha_list[0] is not None and alpha_list[1] is not None, \"Alpha has not been filled\"\n",
    "chk1 = isinstance(alpha_list[0], (int, float))\n",
    "chk2 = isinstance(alpha_list[1], (int, float))\n",
    "assert chk1 and chk2, \"Alpha must be number\"\n",
    "assert theta_initial_list[0] is not None and theta_initial_list[1] is not None, \"initialized theta has not been filled\"\n",
    "chk1 = isinstance(theta_initial_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(theta_initial_list[1], (list,np.ndarray))\n",
    "assert chk1 and chk2, \"Theta must be list\"\n",
    "chk1 = ((n+1, 1) == theta_initial_list[0].shape)\n",
    "chk2 = ((n+1, 1) == theta_initial_list[1].shape)\n",
    "assert chk1 and chk2, \"Theta size are incorrect\"\n",
    "assert num_iters is not None and isinstance(num_iters, int), \"num_iters must be integer\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (5 points)\n",
    "\n",
    "Fill in the code required to train your model on a particular $\\alpha$ and $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea4f55cd93441770beebd895608e4e7",
     "grade": false,
     "grade_id": "cell-77a540a2a0cc2031",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = np.loadtxt('ex2data1.txt',delimiter = ',')\n",
    "\n",
    "# Extracting X and y from normalized data\n",
    "exam1_data = data[:,0]\n",
    "exam2_data = data[:,1]\n",
    "X = np.array([exam1_data, exam2_data]).T\n",
    "y = data[:,2]\n",
    "\n",
    "# Partioning the dataset\n",
    "m, n = X.shape\n",
    "XX = np.insert(X, 0, 1, axis=1)\n",
    "y = y.reshape(m, 1)\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];\n",
    "print(\"y_train:\")\n",
    "print(y_train.shape)\n",
    "\n",
    "# grade task: change 'None, None' value to number(s) or function\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "#         print(alpha,theta_initial)\n",
    "        theta_i, j_history_i = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)\n",
    "        \n",
    "# print(j_history_list[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d8b32e551ac03e7e946f2c677e92d0",
     "grade": true,
     "grade_id": "cell-57627ff7e32cd714",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "assert theta_list[0] is not None and j_history_list[0] is not None, \"No values in theta_list or j_history_list\"\n",
    "chk1 = isinstance(theta_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(j_history_list[0][0], (int, float))\n",
    "assert chk1 and chk2, \"Wrong type in theta_list or j_history_list\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3 (10 points)\n",
    "\n",
    "Write code to plot loss curves for each of the sequences in `j_history_list` from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f63a65931d5f3da4e50c7cd36990e0f",
     "grade": true,
     "grade_id": "cell-33ed3657769adb04",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(j_history_list[0], label='alpha=0.0008, theta=16,6,9')\n",
    "plt.plot(j_history_list[1], label='alpha=0.0008, theta=15,-5,5')\n",
    "plt.plot(j_history_list[2], label='alpha=0.001, theta=16,6,9')\n",
    "plt.plot(j_history_list[3], label='alpha=0.001, theta=15,-5,5')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f75ff7e6c6cd980abee60df22fe5c731",
     "grade": false,
     "grade_id": "cell-59c862747f0fb871",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.4 (10 points)\n",
    "\n",
    "- Repeat your training, but **normalize** your data before training\n",
    "- Compare the results between normalized data and unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of data\n",
    "def normalized_data(data):\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    return (data - means) / stds\n",
    "\n",
    "print(\"Shape of X is:\")\n",
    "print(X.shape)\n",
    "# Normalizing X\n",
    "X_norm = normalized_data(X)\n",
    "print(X_norm.shape)\n",
    "\n",
    "# Partioning the dataset\n",
    "XX = np.insert(X_norm, 0, 1, axis=1)\n",
    "\n",
    "X_train_norm = XX[train_idx,:];\n",
    "X_test_norm = XX[test_idx,:];\n",
    "\n",
    "#Redeclaring the alphas\n",
    "alpha1 = 0.0008\n",
    "alpha2 = 0.001\n",
    "alpha3 = 0.01\n",
    "\n",
    "alpha_list = [alpha1, alpha2, alpha3]\n",
    "\n",
    "num_iters = 100000\n",
    "\n",
    "#Redeclarling the thetas\n",
    "theta_initial1 = np.array([[16],[6],[9]])\n",
    "theta_initial2 = np.array([[15],[-5],[5]])\n",
    "\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "\n",
    "# Repeating the training\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        theta_i, j_history_i = train(X_train_norm, y_train, theta_initial, alpha, num_iters)\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)\n",
    "        \n",
    "# Plotting the loss curves for normalized data\n",
    "plt.plot(j_history_list[0], label='alpha=0.0008, theta=16,6,9')\n",
    "plt.plot(j_history_list[1], label='alpha=0.0008, theta=15,-5,5')\n",
    "plt.plot(j_history_list[2], label='alpha=0.001, theta=16,6,9')\n",
    "plt.plot(j_history_list[3], label='alpha=0.001, theta=15,-5,5')\n",
    "plt.plot(j_history_list[4], label='alpha=0.01, theta=16,6,9')\n",
    "plt.plot(j_history_list[5], label='alpha=0.01, theta=-15,-5,5')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (normalization)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1cd115f866825c9970f827ee43d67f",
     "grade": false,
     "grade_id": "cell-43c3a360c938e4be",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.5 (5 points)\n",
    "\n",
    "Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs plotted above we can see that, as the learning rate is increased and the initial theta is taken closer to the optimized thetas, the graph converges faster. \n",
    "\n",
    "Furthermore, it can also be concluded from the results that the normalized data converges much quicker than the unnormalized data.The graph of  unnormalized data shows how the cost converges only after 400 thousand iterations while the graph for normalized data starts converging at around 10 thousand iterations. \n",
    "\n",
    "We can also see from the unnormalized graph that initial thetas do not make much difference for higher number iterations. However, when using lower number of iterations, theta taken closer to the optimal theta helps in converging the cost faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic regression decision boundary\n",
    "\n",
    "Note that when $\\theta^\\top \\textbf{x} = 0$, we have $h_\\theta(\\textbf{x}) = 0.5$. That is, we are\n",
    "equally unsure as to whether $\\textbf{x}$ belongs to class 0 or class 1. The contour at which\n",
    "$h_\\theta(\\textbf{x}) = 0.5$ is called the classifier's *decision boundary*.\n",
    "\n",
    "We know that in the plane, the equation $$ax+by+c=0$$ is the general form of a 2D line. In our case, we have\n",
    "$$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$$ as our decision boundary, but clearly, this is just a 2D line\n",
    "in the plane. So when we plot $x_1$ against $x_2$, it is easy to plot the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(X, theta):\n",
    "    v_orthogonal = np.array([[theta[1,0]],[theta[2,0]]])\n",
    "    v_ortho_length = np.sqrt(v_orthogonal.T @ v_orthogonal)\n",
    "    dist_ortho = theta[0,0] / v_ortho_length\n",
    "    v_orthogonal = v_orthogonal / v_ortho_length\n",
    "    v_parallel = np.array([[-v_orthogonal[1,0]],[v_orthogonal[0,0]]])\n",
    "    projections = X @ v_parallel\n",
    "    proj_1 = min(projections)\n",
    "    proj_2 = max(projections)\n",
    "    point_1 = proj_1 * v_parallel - dist_ortho * v_orthogonal\n",
    "    point_2 = proj_2 * v_parallel - dist_ortho * v_orthogonal\n",
    "    return point_1, point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(5,5)) \n",
    "ax = plt.axes() \n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Logistic regression boundary')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(X[:,0][idx_0], X[:,1][idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(X[:,0][idx_1], X[:,1][idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "point_1, point_2 = boundary_points(X, theta)\n",
    "plt.plot([point_1[0,0], point_2[0,0]],[point_1[1,0], point_2[1,0]], 'g-')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You may have to adjust the above code to make it work with normalized data.\n",
    "\n",
    "### Test set performance\n",
    "\n",
    "Now let's apply the learned classifier to the test data we reserved in the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    return 1 - np.square(y - y_pred).sum() / np.square(y - y.mean()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_soft = h(X_test, theta)\n",
    "y_test_pred_hard = (y_test_pred_soft > 0.5).astype(int)\n",
    "\n",
    "test_rsq_soft = r_squared(y_test, y_test_pred_soft)\n",
    "test_rsq_hard = r_squared(y_test, y_test_pred_hard)\n",
    "test_acc = (y_test_pred_hard == y_test).astype(int).sum() / y_test.shape[0]\n",
    "\n",
    "print('Got test set soft R^2 %0.4f, hard R^2 %0.4f, accuracy %0.2f' % (test_rsq_soft, test_rsq_hard, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, accuracy is probably the more useful measure of goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Loan prediction dataset\n",
    "\n",
    "Let's take another example dataset and see what we can do with it.\n",
    "\n",
    "This dataset is from [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset).\n",
    "\n",
    "The data concern loan applications. It has 12 independent variables, including 5 categorical variables. The dependent variable is the decision \"Yes\" or \"No\" for extending a loan to an individual who applied.\n",
    "\n",
    "One thing we will have to do is to clean the data, by filling in missing values and converting categorical data to reals.\n",
    "We will use the Python libraries pandas and sklearn to help with the data cleaning and preparation.\n",
    "\n",
    "### Read the data and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas. You may need to run \"pip3 install pandas\" at the console if it's not already installed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "\n",
    "data_train = pd.read_csv('train_LoanPrediction.csv')\n",
    "data_test = pd.read_csv('test_LoanPrediction.csv')\n",
    "\n",
    "# Start to explore the data\n",
    "\n",
    "print('Training data shape', data_train.shape)\n",
    "print('Test data shape', data_test.shape)\n",
    "\n",
    "print('Training data:\\n', data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training and test data\n",
    "\n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())\n",
    "print('Missing values for test data \\n ------------------------\\n', data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values\n",
    "\n",
    "We can see from the above table that the `Married` column has 3 missing values in the training dataset and 0 missing values in the test dataset.\n",
    "Let's take a look at the distribution over the datasets then fill in the missing values in approximately the same ratio.\n",
    "\n",
    "You may be interested to look at the [documentation of the Pandas `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). It's great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio of each category value\n",
    "# Divide the missing values based on ratio\n",
    "# Fillin the missing values\n",
    "# Print the values before and after filling the missing values for confirmation\n",
    "\n",
    "print(data_train['Married'].value_counts())\n",
    "\n",
    "married = data_train['Married'].value_counts()\n",
    "print('Elements in Married variable', married.shape)\n",
    "print('Married ratio ', married[0]/sum(married.values))\n",
    "\n",
    "def fill_martial_status(data, yes_num_train, no_num_train):        \n",
    "    data['Married'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "    data['Married'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "fill_martial_status(data_train, 2, 1)\n",
    "print(data_train['Married'].value_counts()) \n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of examples missing the `Married` attribute is 0.\n",
    "\n",
    "Let's complete the data processing based on examples given and logistic regression model on training dataset. Then we'll get the model's accuracy (goodness of fit) on the test dataset.\n",
    "\n",
    "Here is another example of filling in missing values for the `Dependents` (number of children and other dependents)\n",
    "attribute. We see that categorical values are all numeric except one value \"3+\"\n",
    "Let's create a new category value \"4\" for \"3+\" and ensure that all the data is numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "dependent = data_train['Dependents'].value_counts()\n",
    "\n",
    "print('Dependent ratio 1 ', dependent['0'] / sum(dependent.values))\n",
    "print('Dependent ratio 2 ', dependent['1'] / sum(dependent.values))\n",
    "print('Dependent ratio 3 ', dependent['2'] / sum(dependent.values))\n",
    "print('Dependent ratio 3+ ', dependent['3+'] / sum(dependent.values))\n",
    "\n",
    "def fill_dependent_status(num_0_train, num_1_train, num_2_train, num_3_train, num_0_test, num_1_test, num_2_test, num_3_test):        \n",
    "    data_train['Dependents'].fillna('0', inplace=True, limit = num_0_train)\n",
    "    data_train['Dependents'].fillna('1', inplace=True, limit = num_1_train)\n",
    "    data_train['Dependents'].fillna('2', inplace=True, limit = num_2_train)\n",
    "    data_train['Dependents'].fillna('3+', inplace=True, limit = num_3_train)\n",
    "    data_test['Dependents'].fillna('0', inplace=True, limit = num_0_test)\n",
    "    data_test['Dependents'].fillna('1', inplace=True, limit = num_1_test)\n",
    "    data_test['Dependents'].fillna('2', inplace=True, limit = num_2_test)\n",
    "    data_test['Dependents'].fillna('3+', inplace=True, limit = num_3_test)\n",
    "\n",
    "fill_dependent_status(9, 2, 2, 2, 5, 2, 2, 1)\n",
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "\n",
    "# Convert category value \"3+\" to \"4\"\n",
    "\n",
    "data_train['Dependents'].replace('3+', 4, inplace = True)\n",
    "data_test['Dependents'].replace('3+', 4, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once missing values are filled in, you'll want to convert strings to numbers.\n",
    "\n",
    "Finally, here's an example of replacing missing values for a numeric attribute. Typically, we would use the mean of the attribute over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['LoanAmount'].value_counts())\n",
    "\n",
    "LoanAmt = data_train['LoanAmount'].value_counts()\n",
    "\n",
    "print('mean loan amount ', np.mean(data_train[\"LoanAmount\"]))\n",
    "\n",
    "loan_amount_mean = np.mean(data_train[\"LoanAmount\"])\n",
    "\n",
    "data_train['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 22)\n",
    "data_test['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338b0327bbf4b37bb74ca23c46b81e52",
     "grade": false,
     "grade_id": "cell-2e5822ee2e432273",
     "locked": true,
     "points": 65,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Take-home exercise (65 points)\n",
    "\n",
    "Using the data from Example 2 above, finish the data cleaning and\n",
    "preparation. Build a logistic regression model based on the\n",
    "cleaned dataset and report the accuracy on the test and training sets.\n",
    "\n",
    "- Set up $\\mathbf{x}$ and $y$ data (10 points)\n",
    "- Train a logistic regression model and return the values of $\\theta$ and $J$ you obtained. Find the best $\\alpha$ you can; you may find it best to normalize before training. (30 points)\n",
    "- Using the best model parameters $\\theta$ you can find, run on the test set and get the model's accuracy. (10 points)\n",
    "- Summarize what you did to find the best results in this take home exercise. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To turn in\n",
    "\n",
    "Turn in this Jupyter notebook with your solutions to he exercises and your experiment reports,\n",
    "both for the in-lab exercise and the take-home exercise. Be sure you've discussed what\n",
    "you learned in terms of normalization and data cleaning and the results\n",
    "you obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning for Gender\n",
    "\n",
    "print(data_train['Gender'].value_counts())\n",
    "\n",
    "gender = data_train['Gender'].value_counts()\n",
    "print('male ratio ', gender[0]/sum(gender.values))\n",
    "\n",
    "def fill_gender_status(data, male_num_train, female_num_train):        \n",
    "    data['Gender'].fillna('Male', inplace = True, limit = male_num_train)\n",
    "    data['Gender'].fillna('Female', inplace = True, limit = female_num_train)  \n",
    "\n",
    "fill_gender_status(data_train, 11, 2)\n",
    "fill_gender_status(data_test, 9, 2)\n",
    "\n",
    "#Data cleaning for Sef Employed\n",
    "#Filling the spaces with median value\n",
    "\n",
    "print(data_train['Loan_Amount_Term'].value_counts())\n",
    "\n",
    "LoanAmtTerm = data_train['Loan_Amount_Term'].value_counts()\n",
    "loan_amount_term_median = data_train[\"Loan_Amount_Term\"].median()\n",
    "print(\"Median is\", loan_amount_term_median)\n",
    "\n",
    "data_train['Loan_Amount_Term'].fillna(loan_amount_term_median, inplace=True, limit = 14)\n",
    "data_test['Loan_Amount_Term'].fillna(loan_amount_term_median, inplace=True, limit = 6)\n",
    "print(data_train['Loan_Amount_Term'].value_counts()) \n",
    "\n",
    "#Data cleaning for Sef Employed\n",
    "\n",
    "print(data_train['Self_Employed'].value_counts())\n",
    "\n",
    "self_employed = data_train['Self_Employed'].value_counts()\n",
    "print('Elements in Self Employed variable', self_employed.shape)\n",
    "print('self_employed ratio ', self_employed[1]/sum(self_employed.values))\n",
    "\n",
    "def fill_self_employed_data(data, yes_num_train, no_num_train):        \n",
    "    data['Self_Employed'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "    data['Self_Employed'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "fill_self_employed_data(data_train, 5, 27)\n",
    "fill_self_employed_data(data_test, 3, 20)\n",
    "print(data_train['Self_Employed'].value_counts()) \n",
    "\n",
    "#Data cleaning for Credit history\n",
    "\n",
    "credit_history = data_train['Credit_History'].value_counts()\n",
    "print(credit_history)\n",
    "print('True ratio ', credit_history[1]/sum(credit_history.values))\n",
    "\n",
    "def fill_credit_history_data(data, true_num_train, false_num_train):        \n",
    "    data['Credit_History'].fillna(1, inplace = True, limit = true_num_train)\n",
    "    data['Credit_History'].fillna(0, inplace = True, limit = false_num_train)  \n",
    "\n",
    "fill_credit_history_data(data_train, 42, 8)\n",
    "fill_credit_history_data(data_test, 24, 5)\n",
    "\n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())\n",
    "print('Missing values for train data:\\n------------------------\\n', data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for logistic regression\n",
    "# Converting gender variable with numerical (male is 1 and female is 0)\n",
    "\n",
    "data_train['Gender'].replace('Male', 1, inplace = True)\n",
    "data_train['Gender'].replace('Female', 0, inplace = True)\n",
    "data_test['Gender'].replace('Male', 1, inplace = True)\n",
    "data_test['Gender'].replace('Female', 0, inplace = True)\n",
    "\n",
    "# Converting Married variable with numerical (Yes is 1 and No is 0)\n",
    "\n",
    "data_train['Married'].replace('Yes', 1, inplace = True)\n",
    "data_train['Married'].replace('No', 0, inplace = True)\n",
    "data_test['Married'].replace('Yes', 1, inplace = True)\n",
    "data_test['Married'].replace('No', 0, inplace = True)\n",
    "\n",
    "\n",
    "# Converting Education variable with numerical (Graduate is 1 and Not Graduate is 0)\n",
    "\n",
    "data_train['Education'].replace('Graduate', 1, inplace = True)\n",
    "data_train['Education'].replace('Not Graduate', 0, inplace = True)\n",
    "data_test['Education'].replace('Graduate', 1, inplace = True)\n",
    "data_test['Education'].replace('Not Graduate', 0, inplace = True)\n",
    "\n",
    "# Converting Self_Employed variable with numerical (Yes is 1 and No is 0)\n",
    "\n",
    "data_train['Self_Employed'].replace('Yes', 1, inplace = True)\n",
    "data_train['Self_Employed'].replace('No', 0, inplace = True)\n",
    "data_test['Self_Employed'].replace('Yes', 1, inplace = True)\n",
    "data_test['Self_Employed'].replace('No', 0, inplace = True)\n",
    "\n",
    "# Converting Property_Area variable with numerical (Rural is 0 Semiurban is 1 and Urban is 2)\n",
    "\n",
    "data_train['Property_Area'].replace('Rural', 0, inplace = True)\n",
    "data_train['Property_Area'].replace('Semiurban', 1, inplace = True)\n",
    "data_train['Property_Area'].replace('Urban', 2, inplace = True)\n",
    "\n",
    "data_test['Property_Area'].replace('Rural', 0, inplace = True)\n",
    "data_test['Property_Area'].replace('Semiurban', 1, inplace = True)\n",
    "data_test['Property_Area'].replace('Urban', 2, inplace = True)\n",
    "\n",
    "# Converting Loan_Status variable with numerical (Y is 1 and N is 0)\n",
    "\n",
    "data_train['Loan_Status'].replace('Y', 1, inplace = True)\n",
    "data_train['Loan_Status'].replace('N', 0, inplace = True)\n",
    "\n",
    "\n",
    "# Converting Dependents variable string with numerical ('1' is 1, '2' is 2, '4' is 3)\n",
    "\n",
    "data_train['Dependents'].replace('0', 0, inplace = True)\n",
    "data_train['Dependents'].replace('1', 1, inplace = True)\n",
    "data_train['Dependents'].replace('2', 2, inplace = True)\n",
    "data_train['Dependents'].replace('4', 3, inplace = True)\n",
    "\n",
    "data_test['Dependents'].replace('0', 0, inplace = True)\n",
    "data_test['Dependents'].replace('1', 1, inplace = True)\n",
    "data_test['Dependents'].replace('2', 2, inplace = True)\n",
    "data_test['Dependents'].replace('4', 3, inplace = True)\n",
    "\n",
    "# removing loan ID \n",
    "data_train=data_train.drop(['Loan_ID'],axis = 1)\n",
    "data_test=data_test.drop(['Loan_ID'],axis = 1)\n",
    "\n",
    "\n",
    "print(data_test)\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating X and y in train data\n",
    "data_train = np.array(data_train)\n",
    "X = data_train[:,:-1]\n",
    "y = data_train[:,-1]\n",
    "\n",
    "m ,n = X.shape\n",
    "print(m,n)\n",
    "\n",
    "print(data_train)\n",
    "\n",
    "#Dividing given training set to get validation set \n",
    "percent_train = .7\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "validation_idx = idx[m_train:]\n",
    "\n",
    "X_train = X[train_idx,:]\n",
    "X_validation = X[validation_idx,:]\n",
    "\n",
    "y_train = y[train_idx].reshape(-1,1)\n",
    "y_validation = y[validation_idx]\n",
    "\n",
    "#Normalizing X\n",
    "XX_train_norm = normalized_data(X_train)\n",
    "X_train_norm = np.insert(XX_train_norm, 0, 1, axis=1)\n",
    "X_validation = np.insert(X_validation, 0, 1, axis=1)\n",
    "print(X_train_norm)\n",
    "print(X_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing alpha and iteration\n",
    "alpha = .0005\n",
    "num_iters = 100000\n",
    "\n",
    "theta_initial = np.zeros((n+1, 1)).reshape(-1,1)\n",
    "print(X_train_norm.shape)\n",
    "print(theta_initial.shape)\n",
    "\n",
    "theta, j_history = train(X_train_norm, y_train, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(\"Theta optimized:\", theta)\n",
    "print(\"Cost with optimized theta:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss curve\n",
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with increased learning rate and different theta and iteration\n",
    "alpha1 = 0.0009\n",
    "alpha2 = 0.001\n",
    "\n",
    "#initializing thetas\n",
    "theta_initial1 = np.full((12,),1).reshape(-1,1)\n",
    "theta_initial2 = np.full((12,),0).reshape(-1,1)\n",
    "\n",
    "num_iters = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [alpha1, alpha2]\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        theta_i, j_history_i = train(X_train_norm, y_train, theta_initial, alpha, num_iters)\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)\n",
    "print(theta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss curves for normalized data\n",
    "plt.plot(j_history_list[0], label='alpha=0.0009, theta=1,1,1..')\n",
    "plt.plot(j_history_list[1], label='alpha=0.0009, theta=0,0,0..')\n",
    "plt.plot(j_history_list[2], label='alpha=0.001, theta=1,1,1..')\n",
    "plt.plot(j_history_list[3], label='alpha=0.001, theta=0,0,0..')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (normalization)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    return 1 - np.square(y - y_pred).sum() / np.square(y - y.mean()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.errstate(over='ignore'):\n",
    "    #using validation data set for determining the accuracy\n",
    "    y_validation_pred_soft = h(X_validation, theta)\n",
    "    y_validation_pred_hard = (y_validation_pred_soft > 0.5).astype(int)\n",
    "\n",
    "    validation_rsq_soft = r_squared(y_validation, y_validation_pred_soft)\n",
    "    validation_rsq_hard = r_squared(y_validation, y_validation_pred_hard)\n",
    "    validation_acc = (y_validation_pred_hard == y_validation).astype(int).sum() / y_validation.shape[0]\n",
    "\n",
    "    print('Got validation set soft R^2 %0.4f, hard R^2 %0.4f, accuracy %0.2f' % (test_rsq_soft, test_rsq_hard, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.errstate(over='ignore'):\n",
    "    #Using given test set to just predict y\n",
    "    XX_test = np.array(data_test)\n",
    "    X_test = np.insert(XX_test, 0, 1, axis=1)\n",
    "\n",
    "    y_test_pred_soft = h(X_test, theta)\n",
    "    y_test_pred_hard = (y_test_pred_soft > 0.5).astype(int)\n",
    "    \n",
    "    #Printing only 5 values of y\n",
    "    print(\"y_test_pred_soft\", y_test_pred_soft[24:29,])\n",
    "    print(\"y_test_pred_hard\", y_test_pred_hard[24:29,])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "For this take-home exercise, at first the data was cleaned and it was prepared for further processing. Then the training data set was divided into two parts: a training set and a validation set, so that the model's accuracy could be tested with the validation set. Following that, the value of alpha was set to 0.0005 and the model was iterated 100000 times with initial thetas set to zeroes to obtain optimized theta and cost. A graph was also created for the same.\n",
    "\n",
    "To get a better model output, the learning rate was adjusted to 0.0009 and 0.001 and two initial thetas (arrays of zeros and ones) were taken. The model was trained for these values and the graph for the same may be seen above.\n",
    "\n",
    "The model's accuracy was then calculated using the various values of optimal thetas that was obtained from all of these iterations for various alphas and initial thetas. When employing alpha 0.0005 and theta zeroes, the model's accuracy was found to be around 93 percent. When using alpha 0.001 with theta zeros, similar model correctness was seen. \n",
    "\n",
    "Finally, using the test data, the value of y (that is y test pred soft and y test pred hard) was computed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
